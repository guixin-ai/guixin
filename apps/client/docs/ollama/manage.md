# 管理模型

成功下载模型后，您需要学习如何有效地管理这些模型。本指南将介绍在硅信中查看、使用和删除 Ollama 模型的方法，以及一些实用的模型管理技巧。

## 查看已安装模型

在硅信中查看所有已安装的 Ollama 模型：

1. 进入"设置" > "Ollama 本地模型管理"
2. 在页面中部可以看到"已安装模型"列表
3. 此列表显示所有本地安装的模型，包括：
   - 模型名称
   - 模型大小
   - 下载/修改日期
   - 标签（包含参数量和量化级别等信息）

## 查看模型详情

要查看某个模型的详细信息：

1. 在"已安装模型"列表中找到目标模型
2. 点击模型名称右侧的"详情"按钮
3. 在弹出的详情窗口中，您可以查看：
   - 模型架构
   - 参数量大小
   - 量化级别
   - 许可证信息
   - 模型描述和适用场景
   - 存储位置

## 设置默认模型

将某个模型设置为默认使用的模型：

1. 在"已安装模型"列表中找到要设置为默认的模型
2. 点击模型名称旁边的"设为默认"按钮
3. 确认后，该模型会被标记为默认模型
4. 默认模型会在聊天界面中自动选中

## 在会话中切换模型

在聊天过程中切换使用的模型：

1. 打开或创建一个聊天窗口
2. 在聊天窗口顶部，找到模型选择下拉菜单
3. 点击下拉菜单，会显示所有可用的模型列表
4. 选择要切换的模型
5. 系统会自动加载新选择的模型，之后的对话将使用新模型

## 删除模型

删除不再需要的模型以释放空间：

1. 在"已安装模型"列表中找到要删除的模型
2. 点击模型名称右侧的"删除"按钮
3. 在确认对话框中点击"确认"
4. 系统会删除该模型文件，并从列表中移除

**注意**：删除模型操作不可逆，如果需要再次使用，需要重新下载模型。

## 模型使用技巧

### 了解模型特性

不同模型有不同的特点和专长：

- **参数量较大的模型**（如 13B 以上）通常推理能力更强，但速度较慢
- **量化程度高的模型**（如 q4_0）速度快，内存占用小，但可能影响精度
- **专门指令微调的模型**（带有 instruct 标记）更适合执行指令
- **对话模型**（带有 chat 标记）更适合自然交流

### 有效提示技巧

根据模型类型采取不同的提示策略：

- 对于 instruct 类模型，清晰明确的指令效果更好
- 对于 chat 类模型，可以采用对话式输入
- 尝试提供一些示例说明您期望的输出格式
- 复杂任务可拆分为多个简单步骤逐步提问

### 管理上下文长度

合理管理对话上下文可以提高模型响应质量：

- 大多数模型有 2K-8K token 的上下文窗口限制
- 对话过长时，可以使用"清除上下文"功能开始新的对话
- 关键信息应当放在提示的开头或结尾，中间部分更容易被忽略
- 使用"新建对话"而非一直在同一对话中提问不同主题

## 监控模型性能

### 内存使用

监控模型的内存使用情况：

1. 在使用模型过程中，查看系统任务管理器
2. 正常情况下，Ollama 进程会占用 4GB-20GB 内存（取决于模型大小）
3. 如果内存使用异常，可能需要尝试更小的模型或更高的量化级别

### 生成速度

了解影响生成速度的因素：

- 首次加载模型时会较慢，后续使用会更快
- CPU 模式下，小模型通常能达到 1-20 token/s
- GPU 加速下，速度可提升 3-10 倍
- 如需更快的响应，可以尝试：
  - 使用更小的模型
  - 选择更高量化级别的模型
  - 启用 GPU 加速（如果可用）

## 高级模型管理

### 使用命令行管理

高级用户可以使用 Ollama 命令行工具进行模型管理：

**列出所有模型**：

```bash
ollama list
```

**删除指定模型**：

```bash
ollama rm model_name:tag
```

### 模型备份

备份重要模型以避免重复下载：

1. 找到模型存储位置（见[下载模型](/ollama/download)章节）
2. 复制模型文件夹到安全位置
3. 如需恢复，将备份文件复制回原位置

## 下一步

了解模型管理后，您可以：

1. [性能优化](/ollama/performance) - 优化模型运行性能
2. [常见问题](/ollama/faq) - 解答使用过程中的疑问
