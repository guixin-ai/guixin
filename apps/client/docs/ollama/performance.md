# 性能优化

本指南将帮助您优化 Ollama 模型的运行性能，使模型在您的硬件条件下获得最佳体验。无论您使用的是入门级笔记本还是高端工作站，合理的优化都能显著提升模型的响应速度和稳定性。

## 硬件资源优化

### CPU 优化

1. **关闭不必要的应用程序**

   - 运行 Ollama 模型时，关闭其他占用 CPU 资源的程序
   - 特别是浏览器、视频编辑软件等重度消耗资源的应用

2. **设置 CPU 线程数**

   - 通过环境变量 `OLLAMA_NUM_THREADS` 设置使用的 CPU 线程数
   - 建议值：物理核心数的 75%-100%（例如：8 核 CPU 设置为 6-8）

3. **调整进程优先级**
   - Windows：任务管理器中将 Ollama 进程设为"高于标准"优先级
   - macOS：使用 `nice` 命令调整优先级
   - Linux：使用 `nice` 或 `renice` 命令调整优先级

### 内存管理

1. **选择合适的模型大小**

   - 内存小于 8GB：优先选择 7B 及以下参数的模型
   - 内存 8GB-16GB：可使用 7B-13B 参数的模型
   - 内存 16GB 以上：可尝试更大的模型如 34B 或 70B

2. **使用量化模型**

   - 优先选择 q4_0 或 q4_1 量化的模型，可减少 70-80% 内存使用
   - 在精度要求不高的场景，甚至可使用 q3 或 q2 量化模型

3. **限制模型缓存**
   - 通过设置环境变量 `OLLAMA_MODEL_CACHE` 限制模型缓存大小
   - 例如：`OLLAMA_MODEL_CACHE=2048` 将缓存限制为 2GB

### GPU 加速

1. **启用 GPU 支持**

   - 设置环境变量 `OLLAMA_GPU_LAYERS` 控制 GPU 加速层数
   - 数值越大，GPU 加速效果越明显，但需要更多显存

2. **显存控制**
   - NVIDIA 显卡：可通过 `CUDA_VISIBLE_DEVICES` 指定使用的 GPU
   - 对于显存有限的 GPU，可降低 `OLLAMA_GPU_LAYERS` 数值
   - 入门级显卡(4GB显存)建议值: 24-32
   - 中等显卡(8GB显存)建议值: 48-64
   - 高端显卡(16GB+显存)可使用更高值

## 环境变量设置

### Windows 设置环境变量

1. 右键点击"此电脑"→"属性"→"高级系统设置"→"环境变量"
2. 在"用户变量"部分点击"新建"
3. 输入变量名（如 `OLLAMA_NUM_THREADS`）和值（如 `8`）
4. 点击"确定"保存
5. 重启 Ollama 服务使设置生效

### macOS 设置环境变量

在终端中运行：

```bash
echo 'export OLLAMA_NUM_THREADS=8' >> ~/.zshrc
echo 'export OLLAMA_GPU_LAYERS=64' >> ~/.zshrc
source ~/.zshrc
```

然后重启 Ollama。

### Linux 设置环境变量

编辑 `~/.bashrc` 或 `~/.zshrc` 文件，添加：

```bash
export OLLAMA_NUM_THREADS=8
export OLLAMA_GPU_LAYERS=64
```

然后执行 `source ~/.bashrc` 并重启 Ollama。

## 模型参数优化

### 温度调节

调整模型生成的创造性和随机性：

- 低温度值（0.1-0.4）：生成更确定、一致的回答，适合事实性任务
- 中温度值（0.5-0.7）：平衡创造性和一致性，适合一般对话
- 高温度值（0.8-1.0）：生成更多样化、创造性的回答，适合创意任务

### 上下文窗口调整

- 减小上下文窗口可降低内存使用，提高处理速度
- 通过 API 或高级设置调整 `num_ctx` 参数
- 对于大多数日常任务，设置为 2048-4096 tokens 通常足够
- 长文档处理可能需要更大的上下文窗口

### 采样参数调整

优化 Top-P 和 Top-K 参数：

- Top-P（0.1-1.0）：控制模型从多少概率质量中选择词元
  - 较低值（0.1-0.5）生成更保守、重复性更高的文本
  - 较高值（0.7-1.0）生成更多样的文本
- Top-K（1-100）：控制模型考虑的词元数量
  - 较低值（5-20）生成更保守的文本
  - 较高值（40-100）生成更多样的文本

## 性能优化建议

### 入门级设备优化

适用于 8GB RAM，集成显卡或入门级独显的设备：

1. 选择 7B 以下小型模型，如 `gemma:2b-instruct-q4_0` 或 `phi-2`
2. 使用高度量化的模型（q3_K_M、q4_0 等）
3. 设置 `OLLAMA_NUM_THREADS` 为 CPU 核心数的一半
4. 关闭所有后台应用程序
5. 减小上下文窗口大小（2048 或更低）
6. 尽量避免长对话，定期开始新会话

### 中端设备优化

适用于 16GB RAM，中端显卡的设备：

1. 可使用 7B-13B 参数的模型，如 `llama3:8b-instruct-q4_0`
2. 设置 `OLLAMA_NUM_THREADS` 为接近 CPU 核心总数
3. 如有 6GB 以上显存的独立显卡，设置 `OLLAMA_GPU_LAYERS=32`
4. 使用 q4_0 或 q5_0 量化级别，平衡性能和精度

### 高端设备优化

适用于 32GB+ RAM，高端显卡的设备：

1. 可使用更大模型，如 `yi:34b` 或 mixtral 系列
2. 对于多 GPU 设置，可以使用 `CUDA_VISIBLE_DEVICES` 指定 GPU
3. 设置较高的 `OLLAMA_GPU_LAYERS` 值（80-100）充分利用 GPU
4. 可尝试使用更低量化级别（q6_K 或 q8_0）获得更高质量输出

## 降低响应延迟

1. **减少提示长度**

   - 使用简洁明了的提示，避免不必要的冗长描述
   - 上下文信息尽量精简，仅保留关键内容

2. **控制生成长度**

   - 设置合理的最大生成长度（`num_predict` 参数）
   - 对于实时对话，可设置较小值如 256-512 tokens

3. **使用流式输出**

   - 硅信默认使用流式输出，可提供更好的实时体验
   - 流式模式下，文本生成过程中即可展示，降低感知延迟

4. **批处理请求**
   - 对于需要处理多个独立任务的场景，考虑批量处理
   - 避免频繁加载/卸载模型，减少总体处理时间

## 高级性能调优

1. **预热请求**

   - 在正式使用前，先发送一个简单的请求"预热"模型
   - 预热后的模型推理速度通常会更快

2. **自定义模型配置**

   - 使用 Modelfile 创建自定义配置的模型
   - 可根据具体使用场景预设参数，如温度、上下文大小等

3. **使用 SSD 存储模型**
   - 将模型文件存储在 SSD 上可显著提高加载速度
   - 使用 `OLLAMA_MODELS` 环境变量指定模型存储位置

## 监控与诊断

### 性能监控工具

- **Windows**: 任务管理器、资源监视器
- **macOS**: 活动监视器、top 命令
- **Linux**: htop、nvidia-smi (适用于 NVIDIA GPU)

### 常见性能问题与解决方案

1. **模型加载缓慢**

   - 原因：模型文件大、磁盘读取速度慢
   - 解决：使用 SSD 存储模型、选择更小的模型、使用量化版本

2. **生成速度慢**

   - 原因：硬件资源不足、设置不当
   - 解决：检查 CPU/GPU 使用情况、调整线程数和 GPU 层数、选择更小模型

3. **内存使用过高**

   - 原因：模型太大、上下文窗口过大
   - 解决：使用更高量化级别的模型、减小上下文窗口、限制缓存大小

4. **GPU 利用率低**
   - 原因：GPU 层数设置不当、驱动问题
   - 解决：增加 `OLLAMA_GPU_LAYERS` 值、更新 GPU 驱动

## 下一步

性能优化后，如果仍有问题，请参考：

- [常见问题](/ollama/faq) - 查看更多故障排除信息
